{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import gensim\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"I love this product amazing quality\",\n",
    "    \"Terrible product poor quality\",\n",
    "    \"I love the amazing service\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into smaller pieces, such as words or sentences.\n",
    "\n",
    "Real-world use case:\n",
    "Used in search engines to split queries into words for matching relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1: ['i', 'love', 'this', 'product', 'amazing', 'quality']\n",
      "Review 2: ['terrible', 'product', 'poor', 'quality']\n",
      "Review 3: ['i', 'love', 'the', 'amazing', 'service']\n"
     ]
    }
   ],
   "source": [
    "tokenized_reviews = [word_tokenize(review.lower()) for review in reviews]\n",
    "for i, tokens in enumerate(tokenized_reviews):\n",
    "    print(f\"Review {i+1}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)\n",
    "BoW represents text data as a vector of word counts.\n",
    "\n",
    "Real-world use case:\n",
    "Used in spam detection, sentiment analysis, and document classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['amazing' 'love' 'poor' 'product' 'quality' 'service' 'terrible' 'the'\n",
      " 'this']\n",
      "BoW Matrix:\n",
      " [[1 1 0 1 1 0 0 0 1]\n",
      " [0 0 1 1 1 0 1 0 0]\n",
      " [1 1 0 0 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(reviews)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Matrix:\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Word2Vec converts words into vector representations based on context.\n",
    "\n",
    "Real-world use case:\n",
    "Used in chatbots, recommendation systems, and search engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'love': [-0.07511634 -0.00929911  0.09538099 -0.07319422 -0.02333676 -0.01937682\n",
      "  0.0807754  -0.05930967  0.00045279 -0.0475374 ]\n",
      "Vector for 'quality': [-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809\n",
      "  0.06458873  0.08972988 -0.05015428 -0.03763372]\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(tokenized_reviews, vector_size=10, window=2, min_count=1, sg=1)\n",
    "print(\"Vector for 'love':\", model.wv['love'])\n",
    "print(\"Vector for 'quality':\", model.wv['quality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Word2Vec\n",
    "This approach averages all word vectors in a sentence to get a single vector.\n",
    "\n",
    "Real-world use case:\n",
    "Used in document similarity, text clustering, and recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1 Average Vector: [-0.00220742  0.0134073   0.01929608]...\n",
      "Review 2 Average Vector: [0.02844399 0.04120733 0.03952274]...\n",
      "Review 3 Average Vector: [-0.03893989  0.01472283 -0.02407057]...\n"
     ]
    }
   ],
   "source": [
    "def get_avg_word2vec(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "avg_vectors = [get_avg_word2vec(tokens, model) for tokens in tokenized_reviews]\n",
    "for i, vec in enumerate(avg_vectors):\n",
    "    print(f\"Review {i+1} Average Vector: {vec[:3]}...\")  # Showing first 3 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "TF-IDF gives importance to words that appear frequently in a document but not across all documents.\n",
    "\n",
    "Real-world use case:\n",
    "Used in search engines, document ranking, and keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['amazing' 'love' 'poor' 'product' 'quality' 'service' 'terrible' 'the'\n",
      " 'this']\n",
      "TF-IDF Matrix:\n",
      " [[0.41779577 0.41779577 0.         0.41779577 0.41779577 0.\n",
      "  0.         0.         0.54935123]\n",
      " [0.         0.         0.5628291  0.42804604 0.42804604 0.\n",
      "  0.5628291  0.         0.        ]\n",
      " [0.42804604 0.42804604 0.         0.         0.         0.5628291\n",
      "  0.         0.5628291  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming (Porter Stemmer)\n",
    "Stemming reduces words to their root form by chopping off suffixes. It doesn't always produce real words but is faster than lemmatization.\n",
    "\n",
    "Use Case:\n",
    "Stemming is used in search engines (reducing words to base form improves matching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEWS\n",
      "['I love this product amazing quality', 'Terrible product poor quality', 'I love the amazing service']\n",
      " \n",
      "Stemmed Output:\n",
      "Review 1 stemmed: ['i', 'love', 'thi', 'product', 'amaz', 'qualiti']\n",
      "Review 2 stemmed: ['terribl', 'product', 'poor', 'qualiti']\n",
      "Review 3 stemmed: ['i', 'love', 'the', 'amaz', 'servic']\n"
     ]
    }
   ],
   "source": [
    "print('REVIEWS')\n",
    "print(reviews)\n",
    "print(' ')\n",
    "print('Stemmed Output:' )\n",
    "ps = PorterStemmer()\n",
    "stemmed_reviews = [[ps.stem(token) for token in tokens] for tokens in tokenized_reviews]\n",
    "for i, stemmed in enumerate(stemmed_reviews):\n",
    "    print(f\"Review {i+1} stemmed: {stemmed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Lemmatization reduces words to their dictionary root form (lemma) using linguistic rules. It considers the word’s meaning, making it more accurate than stemming.\n",
    "\n",
    "Use Case:\n",
    "Lemmatization is used in chatbots, spell-checkers, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEWS\n",
      "['I love this product amazing quality', 'Terrible product poor quality', 'I love the amazing service']\n",
      " \n",
      "Lemmatized Output:\n",
      "Review 1 lemmatized: ['i', 'love', 'this', 'product', 'amazing', 'quality']\n",
      "Review 2 lemmatized: ['terrible', 'product', 'poor', 'quality']\n",
      "Review 3 lemmatized: ['i', 'love', 'the', 'amazing', 'service']\n"
     ]
    }
   ],
   "source": [
    "print('REVIEWS')\n",
    "print(reviews)\n",
    "print(' ')\n",
    "print('Lemmatized Output:' )\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_reviews = [[lemmatizer.lemmatize(token) for token in tokens] for tokens in tokenized_reviews]\n",
    "for i, lemmatized in enumerate(lemmatized_reviews):\n",
    "    print(f\"Review {i+1} lemmatized: {lemmatized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words Removal\n",
    "Stop words (e.g., \"is\", \"and\", \"the\") are common words that don’t add meaning in NLP tasks. We remove them to reduce noise.\n",
    "\n",
    "Use Case:\n",
    "Stop word removal is used in text classification, sentiment analysis, and keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEWS\n",
      "['I love this product amazing quality', 'Terrible product poor quality', 'I love the amazing service']\n",
      " \n",
      "Stop Words Removal Output:\n",
      "Review 1 without stop words: ['love', 'product', 'amazing', 'quality']\n",
      "Review 2 without stop words: ['terrible', 'product', 'poor', 'quality']\n",
      "Review 3 without stop words: ['love', 'amazing', 'service']\n"
     ]
    }
   ],
   "source": [
    "print('REVIEWS')\n",
    "print(reviews)\n",
    "print(' ')\n",
    "print('Stop Words Removal Output:' )\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_reviews = [[token for token in tokens if token not in stop_words] \n",
    "                   for tokens in tokenized_reviews]\n",
    "for i, filtered in enumerate(filtered_reviews):\n",
    "    print(f\"Review {i+1} without stop words: {filtered}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
